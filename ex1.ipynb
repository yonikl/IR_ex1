{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-02T14:54:36.415021Z",
     "start_time": "2024-12-02T14:54:36.058561Z"
    }
   },
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "df = pd.read_excel('posts_first_targil.xlsx', sheet_name=None)"
   ],
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:54:36.506949Z",
     "start_time": "2024-12-02T14:54:36.498169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sheet_name, sheet_df in df.items():\n",
    "    print(f\"Sheet name: {sheet_name}\")\n",
    "    print(sheet_df.columns)"
   ],
   "id": "f7fce9efa774f4b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: A-J\n",
      "Index(['sub_title', 'date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: BBC\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: J-P\n",
      "Index(['date', 'Newspaper', 'Body', 'title'], dtype='object')\n",
      "Sheet name: NY-T\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:54:36.562842Z",
     "start_time": "2024-12-02T14:54:36.557095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we're renaming J-P column to match others\n",
    "df['J-P'].rename(columns={'Body': 'Body Text'}, inplace=True)"
   ],
   "id": "77d65ade27251637",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:54:36.590100Z",
     "start_time": "2024-12-02T14:54:36.582556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sheet_name, sheet_df in df.items():\n",
    "    print(f\"Sheet name: {sheet_name}\")\n",
    "    print(sheet_df.columns)"
   ],
   "id": "4b3564b19c151695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: A-J\n",
      "Index(['sub_title', 'date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: BBC\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: J-P\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n",
      "Sheet name: NY-T\n",
      "Index(['date', 'Newspaper', 'Body Text', 'title'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:54:36.634557Z",
     "start_time": "2024-12-02T14:54:36.628280Z"
    }
   },
   "cell_type": "code",
   "source": "import re",
   "id": "2c5374a2ebdd5ac6",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:54:36.680084Z",
     "start_time": "2024-12-02T14:54:36.674486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    pattern = r\"((?<!\\w)[^\\s\\w]|[^\\s\\w](?!\\w))\"\n",
    "\n",
    "    cleaned_text = re.sub(pattern, r\" \\1 \", text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    return re.sub(r\"\\s+\", \" \", cleaned_text).strip()"
   ],
   "id": "96f0408ddf710f38",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:57:14.988192Z",
     "start_time": "2024-12-02T14:57:13.893579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sheet_name, sheet_df in df.items():\n",
    "    sheet_df = sheet_df.map(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "    sheet_df.to_csv(f'clean_data\\\\{sheet_name}.csv', index=False)"
   ],
   "id": "79c896c01665760f",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Lemmatization",
   "id": "f9d716a6e0bc484d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T19:15:11.924374Z",
     "start_time": "2024-12-02T19:14:16.644215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "\n"
   ],
   "id": "e75b618a40becffa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yoni\n",
      "[nltk_data]     Klein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T18:31:43.555901Z",
     "start_time": "2024-12-02T18:31:43.546711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)  # Tokenize text\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in words])"
   ],
   "id": "f2fad3bee5a5a19e",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T18:32:03.350344Z",
     "start_time": "2024-12-02T18:31:44.439848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "for file in os.listdir('./clean_data'):\n",
    "    print(file)\n",
    "    sheet = pd.read_csv(f'clean_data\\\\{file}')\n",
    "    sheet = sheet.map(lambda x: lemmatize_text(x) if isinstance(x, str) else x)\n",
    "    sheet.to_csv(f\"lemmatize_data\\\\{file}\", index=False)\n",
    "\n"
   ],
   "id": "d5021dcf2ef0b3d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-J.csv\n",
      "BBC.csv\n",
      "J-P.csv\n",
      "NY-T.csv\n"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T20:29:16.830316Z",
     "start_time": "2024-12-02T20:29:16.154480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from scipy.sparse import save_npz\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ],
   "id": "8bd6416c0026cd8b",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T21:48:36.532163Z",
     "start_time": "2024-12-02T21:48:36.524316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_metadata(word_index, output_file):\n",
    "    # save for feature use\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    metadata_df = pd.DataFrame(list(word_index.items()), columns=[\"Word\", \"Index\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    metadata_df.to_csv(output_file, index=False)\n",
    "    print(f\"Metadata saved to {output_file}\")\n"
   ],
   "id": "5d14bef5d38ec171",
   "outputs": [],
   "execution_count": 244
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:08:15.013333Z",
     "start_time": "2024-12-02T22:06:06.514840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# bm25 to the lemmatized docs\n",
    "\n",
    "for file in os.listdir('lemmatize_data'):\n",
    "    print(file)\n",
    "    sheet = pd.read_csv(f'lemmatize_data\\\\{file}')\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()  # Tokenize the text\n",
    "        return [word.lower() for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Build the corpus\n",
    "    if file == 'A-J.csv':\n",
    "        corpus = [\n",
    "            remove_stopwords(f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}')\n",
    "            for _, row in sheet.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus = [\n",
    "            remove_stopwords(f'{row[\"title\"]} {row[\"Body Text\"]}')\n",
    "            for _, row in sheet.iterrows()\n",
    "        ]\n",
    "\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    all_words = bm25.idf.keys()\n",
    "    word_index = {word: idx for idx, word in enumerate(all_words)}\n",
    "    save_metadata(word_index, f'metadata\\\\lemma\\\\{file}')\n",
    "    rows, cols, data = [], [], []\n",
    "    for word in all_words:\n",
    "        scores = bm25.get_scores(word)\n",
    "        for doc_idx, score in enumerate(scores):\n",
    "            if score > 0:  # Only include non-zero scores to keep it sparse\n",
    "                rows.append(doc_idx)  # Document index\n",
    "                cols.append(word_index[word])  # Word index\n",
    "                data.append(score)  # BM25 score\n",
    "\n",
    "\n",
    "    sparse_bm25_matrix = csr_matrix((data, (rows, cols)), shape=(len(corpus), len(all_words)))\n",
    "\n",
    "    save_npz(f'bm25\\\\lemma\\\\{file.split(\".\")[0]}', sparse_bm25_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a8c7ade03967579f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-J.csv\n",
      "Metadata saved to metadata\\lemma\\A-J.csv\n",
      "BBC.csv\n",
      "Metadata saved to metadata\\lemma\\BBC.csv\n",
      "J-P.csv\n",
      "Metadata saved to metadata\\lemma\\J-P.csv\n",
      "NY-T.csv\n",
      "Metadata saved to metadata\\lemma\\NY-T.csv\n"
     ]
    }
   ],
   "execution_count": 258
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:10:47.252148Z",
     "start_time": "2024-12-02T22:08:15.071877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('clean_data'):\n",
    "    print(file)\n",
    "    sheet = pd.read_csv(f'clean_data\\\\{file}')\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()  # Tokenize the text\n",
    "        return [word.lower() for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Build the corpus\n",
    "    if file == 'A-J.csv':\n",
    "        corpus = [\n",
    "            remove_stopwords(f'{row[\"title\"]} {row[\"sub_title\"]} {row[\"Body Text\"]}') for _, row in sheet.iterrows()\n",
    "        ]\n",
    "    else:\n",
    "        corpus = [\n",
    "            remove_stopwords(f'{row[\"title\"]} {row[\"Body Text\"]}') for _, row in sheet.iterrows()\n",
    "        ]\n",
    "\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    all_words = bm25.idf.keys()\n",
    "    word_index = {word: idx for idx, word in enumerate(all_words)}\n",
    "    save_metadata(word_index, f'metadata\\\\clean\\\\{file}')\n",
    "    rows, cols, data = [], [], []\n",
    "    for word in all_words:\n",
    "        scores = bm25.get_scores(word)\n",
    "        for doc_idx, score in enumerate(scores):\n",
    "            if score > 0:  # Only include non-zero scores to keep it sparse\n",
    "                rows.append(doc_idx)  # Document index\n",
    "                cols.append(word_index[word])  # Word index\n",
    "                data.append(score)  # BM25 score\n",
    "\n",
    "\n",
    "    sparse_bm25_matrix = csr_matrix((data, (rows, cols)), shape=(len(corpus), len(all_words)))\n",
    "\n",
    "    save_npz(f'bm25\\\\clean\\\\{file.split(\".\")[0]}', sparse_bm25_matrix)"
   ],
   "id": "9aba26f2b206e5bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-J.csv\n",
      "Metadata saved to metadata\\clean\\A-J.csv\n",
      "BBC.csv\n",
      "Metadata saved to metadata\\clean\\BBC.csv\n",
      "J-P.csv\n",
      "Metadata saved to metadata\\clean\\J-P.csv\n",
      "NY-T.csv\n",
      "Metadata saved to metadata\\clean\\NY-T.csv\n"
     ]
    }
   ],
   "execution_count": 259
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3: IG",
   "id": "77670b981639b51c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:01:08.195017Z",
     "start_time": "2024-12-02T22:01:08.187567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.sparse import load_npz\n",
    "import numpy as np"
   ],
   "id": "12b6c42a5f096490",
   "outputs": [],
   "execution_count": 252
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "IG for lemmatized",
   "id": "42031e0bb3a7470f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:24:29.285411Z",
     "start_time": "2024-12-02T22:24:28.679482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('bm25\\\\lemma'):\n",
    "    # Load the sparse BM25 matrix\n",
    "    sparse_bm25_matrix = load_npz(f'bm25\\\\lemma\\\\{file}')\n",
    "\n",
    "    # Sum the BM25 scores for each word across all documents (proxy for importance)\n",
    "    word_scores = np.array(sparse_bm25_matrix.sum(axis=0)).flatten()  # Sum along columns\n",
    "\n",
    "    # Load word-to-index mapping\n",
    "    word_metadata = pd.read_csv(f\"metadata\\\\lemma\\\\{file.split('.')[0]}.csv\")\n",
    "\n",
    "    # Map scores to words\n",
    "    df_word_scores = pd.DataFrame({\n",
    "        \"Word\": word_metadata[\"Word\"],\n",
    "        \"Score\": word_scores\n",
    "    }).sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_word_scores.to_csv(f\"IG\\\\lemma\\\\{file.split('.')[0]}.csv\", index=False)\n",
    "\n",
    "    # Display top words by score\n",
    "    print(df_word_scores.head())"
   ],
   "id": "ba33fa91057d435c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Word      Score\n",
      "2632  youtu.be/ilery4m9kqc 1526.50330\n",
      "1367                18.5bn 1180.27864\n",
      "1981                   1.4 1171.21460\n",
      "3876                   1.2 1170.98122\n",
      "2788                   1.5 1170.64398\n",
      "           Word      Score\n",
      "9506     17.4.1 4089.23210\n",
      "2594  bbc.co.uk 4082.05305\n",
      "9128   2,332.97 3981.15565\n",
      "2574   07.09.23 3872.42854\n",
      "9126   2,431.29 3648.78138\n",
      "                                               Word      Score\n",
      "21151                            kronishl.e.a.r.h.n 9354.87922\n",
      "21779  yanshufim.giveback.co.il/our_heart_helooking 6448.80153\n",
      "7649           experience.www.noayekutieli.com/ella 6420.32105\n",
      "16348                            15,000-50,000.some 6290.64304\n",
      "16183         www.google-analytics.com/analytics.js 6177.02794\n",
      "                Word      Score\n",
      "5700  3,000-year-old 1031.01485\n",
      "5353     2,000-pound  988.24087\n",
      "3203    35,000people  983.35589\n",
      "4051         570,000  972.24843\n",
      "3192           4,700  965.58694\n"
     ]
    }
   ],
   "execution_count": 261
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "IG for clean",
   "id": "58b6b45561299730"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:24:31.712960Z",
     "start_time": "2024-12-02T22:24:31.126692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in os.listdir('bm25\\\\clean'):\n",
    "    # Load the sparse BM25 matrix\n",
    "    sparse_bm25_matrix = load_npz(f'bm25\\\\clean\\\\{file}')\n",
    "\n",
    "    # Sum the BM25 scores for each word across all documents (proxy for importance)\n",
    "    word_scores = np.array(sparse_bm25_matrix.sum(axis=0)).flatten()  # Sum along columns\n",
    "\n",
    "    # Load word-to-index mapping\n",
    "    word_metadata = pd.read_csv(f\"metadata\\\\clean\\\\{file.split('.')[0]}.csv\")\n",
    "\n",
    "    # Map scores to words\n",
    "    df_word_scores = pd.DataFrame({\n",
    "        \"Word\": word_metadata[\"Word\"],\n",
    "        \"Score\": word_scores\n",
    "    }).sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_word_scores.to_csv(f\"IG\\\\clean\\\\{file.split('.')[0]}.csv\", index=False)\n",
    "\n",
    "    # Display top words by score\n",
    "    print(df_word_scores.head())"
   ],
   "id": "e65c0174503433b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Word      Score\n",
      "2915  youtu.be/ilery4m9kqc 1186.85553\n",
      "1475                18.5bn 1183.80731\n",
      "2173                   1.4 1174.60631\n",
      "4311                   1.2 1174.44120\n",
      "3091                   1.5 1174.09042\n",
      "                                    Word      Score\n",
      "2824           hello.bbclondon@bbc.co.uk 4943.49398\n",
      "16246     northwest.newsonline@bbc.co.uk 4906.26510\n",
      "8490           yorkslincs.news@bbc.co.uk 4902.57845\n",
      "4668          south.newsonline@bbc.co.uk 4901.53774\n",
      "9640   newsonline.westmidlands@bbc.co.uk 4898.11889\n",
      "                                                  Word       Score\n",
      "19278  a.async=1;a.src=g;m.parentnode.insertbefore(a,m 13001.33335\n",
      "19266                           function(i,s,o,g,r,a,m 12224.19119\n",
      "25405                               kronishl.e.a.r.h.n  9288.65067\n",
      "25605   activities&rdquo;emmanuel&nbsp;macron&ldquo;in  8650.33167\n",
      "16591        official&nbsp;osama&nbsp;hamdan&nbsp;said  8573.77903\n",
      "                       Word      Score\n",
      "6394         3,000-year-old 1033.54472\n",
      "4129  gallery,hyperallergic 1027.71666\n",
      "3426           sella,helene 1018.40084\n",
      "4741     speeches,eliciting  993.13563\n",
      "3549           35,000people  985.62684\n"
     ]
    }
   ],
   "execution_count": 262
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
